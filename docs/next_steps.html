<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Next steps &mdash; TopasBayesOpt  documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> TopasBayesOpt
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="worked_examples.html">Worked Examples</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">TopasBayesOpt</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Next steps</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/next_steps.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="next-steps">
<h1>Next steps<a class="headerlink" href="#next-steps" title="Permalink to this headline"></a></h1>
<p>These instructions assume you have already worked through at least one of the worked examples. If you have not, it is strongly recommended that you go back and do so before proceeding.</p>
<p>The following is a list of thing that you may also wish to do with the code:</p>
<section id="restart-an-optimisation">
<h2>Restart an optimisation<a class="headerlink" href="#restart-an-optimisation" title="Permalink to this headline"></a></h2>
<p>Sometimes an optimisation is terminated prematurely because of time limits on job submissions or because your partner turned your computer off.</p>
<p>In such situations, it is easy to restart the optimisation; you just have to change Optimiser.RunOptimisation() to Optimiser.RestartOptimisation().</p>
<p>This can also be used in situations where you initially thought that 20 iterations would be sufficient but later want to extend this to 40 iterations for instance.</p>
</section>
<section id="load-and-interact-with-the-gaussian-process-model">
<h2>Load and interact with the gaussian process model<a class="headerlink" href="#load-and-interact-with-the-gaussian-process-model" title="Permalink to this headline"></a></h2>
<p>One of the nice things about bayesian optimisation is that at the end of it, there is a model which can be used to predict what the objective function might look like at some particular point. Of course whether or not this is useful depends on how well the model was trained, but assuming you have trained a useful model, you can use the logs from a previous run to read in and interact with the gaussian process model. The below script demonstrates this:</p>
</section>
<section id="setting-length-scales-in-the-gaussian-process-model">
<h2>Setting length scales in the gaussian process model<a class="headerlink" href="#setting-length-scales-in-the-gaussian-process-model" title="Permalink to this headline"></a></h2>
<p>Length scales are used in the kernel of the gaussian process model. In simple language, the length scales indicate how close together two points should be to expect them to have a fairly similar value. The length scales are integral in getting a good fit of the gaussian process model to</p>
<p>In this code, the default behaviour is to set the length scales to 10% of the allowed range for each parameter. E.g. if you have a parameter with a lower limit of 1 and an upper limit of 3, the default length scale would be (3-1)*0.1 = 0.2.</p>
<p>This approach works pretty well as the default. However, there may be situations when you wish to apply a little more finesse. In that case, you can also pass an array for length_scales, with one value per parameter, e.g.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Optimiser</span> <span class="o">=</span> <span class="n">to</span><span class="o">.</span><span class="n">BayesianOptimiser</span><span class="p">(</span><span class="n">optimisation_params</span><span class="p">,</span> <span class="n">BaseDirectory</span><span class="p">,</span> <span class="n">SimulationName</span><span class="p">,</span> <span class="n">OptimisationDirectory</span><span class="p">,</span> <span class="n">TopasLocation</span><span class="o">=</span><span class="s1">&#39;~/topas37&#39;</span><span class="p">,</span> <span class="n">ReadMeText</span><span class="o">=</span><span class="n">ReadMeText</span><span class="p">,</span> <span class="n">Overwrite</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">length_scales</span><span class="o">=</span><span class="p">[</span><span class="n">par1_scale</span><span class="p">,</span> <span class="n">par2_scale</span><span class="p">,</span> <span class="n">par3_scale</span><span class="p">,</span> <span class="n">etc</span><span class="o">.</span><span class="p">])</span>
</pre></div>
</div>
<p>ADD IN INFO ABOUT GETTING OPTIMAL PARAMETERS</p>
</section>
<section id="using-different-optimisation-algorithms">
<h2>Using different optimisation algorithms<a class="headerlink" href="#using-different-optimisation-algorithms" title="Permalink to this headline"></a></h2>
<p>The most common optimisation algorithms work on gradients, but when the input is based on monte carlo simulation results, we cannot easily provide derivatives for the objective function, or even assume that it is differentiable.  We believe that in most scenarios, Bayesian optimisation is the ideal approach for this scenario, because it performs well for expensive, noisy, black box objective functions. However, there may be situations where other algorithms perform faster. In particular, if your problem is relatively straight forward it may not be worth the additional overhead of training and understandning Gaussian process models. As such, you can also use some other gradient free optimisation techniques, in particular the Nelder-Mead algorithm and Powell’s method. Both of these use the default <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html">scipy implementation</a>.</p>
<p>Example of switching between different optimisers is below. For detailed documentation on what options are available for the different optimisers, see the code documentaiton. ADD HYPERLINKS!</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Optimiser</span> <span class="o">=</span> <span class="n">to</span><span class="o">.</span><span class="n">BayesianOptimiser</span><span class="p">(</span><span class="n">optimisation_params</span><span class="p">,</span> <span class="n">BaseDirectory</span><span class="p">,</span> <span class="n">SimulationName</span><span class="p">,</span> <span class="n">OptimisationDirectory</span><span class="p">)</span>
<span class="c1"># OR </span>
<span class="n">Optimiser</span> <span class="o">=</span> <span class="n">to</span><span class="o">.</span><span class="n">NealderMeadOptimiser</span><span class="p">(</span><span class="n">optimisation_params</span><span class="p">,</span> <span class="n">BaseDirectory</span><span class="p">,</span> <span class="n">SimulationName</span><span class="p">,</span> <span class="n">OptimisationDirectory</span><span class="p">)</span>
<span class="c1"># OR </span>
<span class="n">Optimiser</span> <span class="o">=</span> <span class="n">to</span><span class="o">.</span><span class="n">PowelOptimiser</span><span class="p">(</span><span class="n">optimisation_params</span><span class="p">,</span> <span class="n">BaseDirectory</span><span class="p">,</span> <span class="n">SimulationName</span><span class="p">,</span> <span class="n">OptimisationDirectory</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="adding-new-optimisation-algorithms">
<h2>Adding new optimisation algorithms<a class="headerlink" href="#adding-new-optimisation-algorithms" title="Permalink to this headline"></a></h2>
<p>It is also possible to add new optimisation algorithms, by inheriting from the optimisation base class, e.g.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">TopasBayesOpt.TopasBayesOpt</span> <span class="kn">import</span> <span class="n">TopasOptBaseClass</span>

<span class="k">class</span> <span class="nc">NewOptimiser</span><span class="p">(</span><span class="n">TopasOptBaseClass</span><span class="p">):</span>
    <span class="c1"># your code here</span>
</pre></div>
</div>
<p>The Base class contains all the basic functionality an optimser will need, such as generating models, reading results, etc. This inheritance mechanism makes it fairly quick to set up new algorithms, e.g. the implementation of the Nelder-Mead algorithm requires only 45 lines of code.</p>
<p>You can look through the source code to see how the different optimisers are implemented.</p>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, Brendan Whelan(s).</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>